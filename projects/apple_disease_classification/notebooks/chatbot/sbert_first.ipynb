{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eerste opzet, wat doet het model? Hoe werkt het?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.37173748e-02 -4.28515524e-02 -1.56286340e-02  1.40537247e-02\n",
      "  3.95537578e-02  1.21796273e-01  2.94333920e-02 -3.17524076e-02\n",
      "  3.54959816e-02 -7.93139860e-02  1.75878499e-02 -4.04369980e-02\n",
      "  4.97259349e-02  2.54912488e-02 -7.18700886e-02  8.14968571e-02\n",
      "  1.47073052e-03  4.79627326e-02 -4.50336188e-02 -9.92174745e-02\n",
      " -2.81769857e-02  6.45046160e-02  4.44670394e-02 -4.76217009e-02\n",
      " -3.52952369e-02  4.38671596e-02 -5.28566092e-02  4.33036505e-04\n",
      "  1.01921476e-01  1.64072067e-02  3.26996371e-02 -3.45986634e-02\n",
      "  1.21339252e-02  7.94870928e-02  4.58343141e-03  1.57778300e-02\n",
      " -9.68204997e-03  2.87625641e-02 -5.05806319e-02 -1.55793764e-02\n",
      " -2.87906770e-02 -9.62280855e-03  3.15556899e-02  2.27348879e-02\n",
      "  8.71449560e-02 -3.85027118e-02 -8.84718224e-02 -8.75498727e-03\n",
      " -2.12343428e-02  2.08923519e-02 -9.02078003e-02 -5.25732227e-02\n",
      " -1.05638849e-02  2.88310796e-02 -1.61455162e-02  6.17836276e-03\n",
      " -1.23234242e-02 -1.07337134e-02  2.83354092e-02 -5.28567620e-02\n",
      " -3.58618200e-02 -5.97989187e-02 -1.09055433e-02  2.91566327e-02\n",
      "  7.97979087e-02 -3.27854039e-04  6.83500664e-03  1.32718338e-02\n",
      " -4.24619876e-02  1.87657028e-02 -9.89234298e-02  2.09049787e-02\n",
      " -8.69605914e-02 -1.50152203e-02 -4.86201867e-02  8.04414675e-02\n",
      " -3.67698376e-03 -6.65044263e-02  1.14556804e-01 -3.04228980e-02\n",
      "  2.96631604e-02 -2.80695129e-02  4.64990363e-02 -2.25513801e-02\n",
      "  8.54223445e-02  3.15446481e-02  7.34541640e-02 -2.21861899e-02\n",
      " -5.29678538e-02  1.27129974e-02 -5.27339764e-02 -1.06188774e-01\n",
      "  7.04731345e-02  2.76736487e-02 -8.05531442e-02  2.39649676e-02\n",
      " -2.65125223e-02 -2.17330977e-02  4.35275622e-02  4.84711900e-02\n",
      " -2.37067696e-02  2.85768434e-02  1.11846134e-01 -6.34936020e-02\n",
      " -1.58318393e-02 -2.26169322e-02 -1.31027419e-02 -1.62067066e-03\n",
      " -3.60929035e-02 -9.78297293e-02 -4.67729196e-02  1.76271517e-02\n",
      " -3.97492200e-02 -1.76390065e-04  3.39627899e-02 -2.09633671e-02\n",
      "  6.33664057e-03 -2.59411260e-02  8.10410306e-02  6.14393353e-02\n",
      " -5.44593576e-03  6.48276061e-02 -1.16844051e-01  2.36860663e-02\n",
      " -1.32058589e-02 -1.12476468e-01  1.90048870e-02 -1.74660621e-34\n",
      "  5.58949970e-02  1.94244813e-02  4.65439111e-02  5.18645719e-02\n",
      "  3.89390700e-02  3.40541154e-02 -4.32113968e-02  7.90637136e-02\n",
      " -9.79530066e-02 -1.27441054e-02 -2.91870628e-02  1.02052242e-02\n",
      "  1.88115872e-02  1.08942576e-01  6.63465336e-02 -5.35295382e-02\n",
      " -3.29228528e-02  4.69827168e-02  2.28883009e-02  2.74114627e-02\n",
      " -2.91982964e-02  3.12706642e-02 -2.22850721e-02 -1.02282256e-01\n",
      " -2.79116649e-02  1.13792904e-02  9.06308442e-02 -4.75414731e-02\n",
      " -1.00718945e-01 -1.23231979e-02 -7.96928555e-02 -1.44636147e-02\n",
      " -7.76400864e-02 -7.66920298e-03  9.73952655e-03  2.24204622e-02\n",
      "  7.77268410e-02 -3.17155616e-03  2.11537946e-02 -3.30394283e-02\n",
      "  9.55247227e-03 -3.73011827e-02  2.61360686e-02 -9.79086012e-03\n",
      " -6.31505251e-02  5.77433826e-03 -3.80031019e-02  1.29684638e-02\n",
      " -1.82499457e-02 -1.56283211e-02 -1.23361172e-03  5.55579215e-02\n",
      "  1.13076443e-04 -5.61256744e-02  7.40165487e-02  1.84452124e-02\n",
      " -2.66368631e-02  1.31951943e-02  7.50086606e-02 -2.46796552e-02\n",
      " -3.24005857e-02 -1.57674905e-02 -8.03516805e-03 -5.61319292e-03\n",
      "  1.05687892e-02  3.26167978e-03 -3.91990282e-02 -9.38677490e-02\n",
      "  1.14227191e-01  6.57304525e-02 -4.72633764e-02  1.45087801e-02\n",
      " -3.54490466e-02 -3.37761641e-02 -5.15505746e-02 -3.81006836e-03\n",
      " -5.15036322e-02 -5.93429320e-02 -1.69415015e-03  7.42107630e-02\n",
      " -4.20091711e-02 -7.19975084e-02  3.17250378e-02 -1.66303366e-02\n",
      "  3.96983884e-03 -6.52750805e-02  2.77391039e-02 -7.51648918e-02\n",
      "  2.27455311e-02 -3.91368307e-02  1.54315727e-02 -5.54908626e-02\n",
      "  1.23318899e-02 -2.59520523e-02  6.66423663e-02 -6.91259647e-34\n",
      "  3.31629068e-02  8.47928971e-02 -6.65583536e-02  3.33541557e-02\n",
      "  4.71611368e-03  1.35361860e-02 -5.38694225e-02  9.20694247e-02\n",
      " -2.96876524e-02  3.16219553e-02 -2.37497203e-02  1.98770911e-02\n",
      "  1.03446200e-01 -9.06947330e-02  6.30626827e-03  1.42886378e-02\n",
      "  1.19293686e-02  6.43722294e-03  4.20104824e-02  1.25344899e-02\n",
      "  3.93019542e-02  5.35691828e-02 -4.30750400e-02  6.10432737e-02\n",
      " -5.39939065e-05  6.91682473e-02  1.05520291e-02  1.22111747e-02\n",
      " -7.23185018e-02  2.50469781e-02 -5.18371463e-02 -4.36562262e-02\n",
      " -6.71818703e-02  1.34828491e-02 -7.25889131e-02  7.04167737e-03\n",
      "  6.58939108e-02  1.08994441e-02 -2.60012317e-03  5.49969152e-02\n",
      "  5.06966822e-02  3.27948816e-02 -6.68833405e-02  6.45557791e-02\n",
      " -2.52075735e-02 -2.92572379e-02 -1.16696738e-01  3.24064642e-02\n",
      "  5.85858300e-02 -3.51756550e-02 -7.15240315e-02  2.24936288e-02\n",
      " -1.00786723e-01 -4.74544726e-02 -7.61963129e-02 -5.87166324e-02\n",
      "  4.21138629e-02 -7.47213811e-02  1.98468156e-02 -3.36500537e-03\n",
      " -5.29736727e-02  2.74729915e-02  3.45736556e-02 -6.11846484e-02\n",
      "  1.06364824e-01 -9.64119881e-02 -4.55945283e-02  1.51489507e-02\n",
      " -5.13525959e-03 -6.64447621e-02  4.31721546e-02 -1.10405711e-02\n",
      " -9.80253145e-03  7.53783211e-02 -1.49570936e-02 -4.80208583e-02\n",
      "  5.80726676e-02 -2.43896767e-02 -2.23137550e-02 -4.36992086e-02\n",
      "  5.12053818e-02 -3.28625776e-02  1.08763322e-01  6.08926974e-02\n",
      "  3.30793508e-03  5.53820245e-02  8.43201503e-02  1.27087133e-02\n",
      "  3.84465717e-02  6.52325526e-02 -2.94683706e-02  5.08005284e-02\n",
      " -2.09347773e-02  1.46135688e-01  2.25561410e-02 -1.77227761e-08\n",
      " -5.02672605e-02 -2.79202010e-04 -1.00328609e-01  2.42811460e-02\n",
      " -7.54043534e-02 -3.79139893e-02  3.96049917e-02  3.10079921e-02\n",
      " -9.05703474e-03 -6.50411770e-02  4.05453108e-02  4.83390279e-02\n",
      " -4.56962287e-02  4.76006093e-03  2.64360872e-03  9.35614482e-02\n",
      " -4.02599499e-02  3.27401944e-02  1.18298242e-02  5.54344989e-02\n",
      "  1.48052230e-01  7.21188933e-02  2.76982290e-04  1.68651473e-02\n",
      "  8.34878627e-03 -8.76156334e-03 -1.33649623e-02  6.14237003e-02\n",
      "  1.57168172e-02  6.94961324e-02  1.08621754e-02  6.08018450e-02\n",
      " -5.33421561e-02 -3.47924009e-02 -3.36272083e-02  6.93906918e-02\n",
      "  1.22987442e-02 -1.45237431e-01 -2.06971960e-03 -4.61132675e-02\n",
      "  3.72749590e-03 -5.59358485e-03 -1.00659840e-01 -4.45953533e-02\n",
      "  5.40921427e-02  4.98892087e-03  1.49534345e-02 -8.26059803e-02\n",
      "  6.26631081e-02 -5.01908036e-03 -4.81857695e-02 -3.53991091e-02\n",
      "  9.03388485e-03 -2.42338032e-02  5.66267557e-02  2.51529124e-02\n",
      " -1.70708876e-02 -1.24780647e-02  3.19518484e-02  1.38421152e-02\n",
      " -1.55814830e-02  1.00178242e-01  1.23657197e-01 -4.22967002e-02]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 5.64524867e-02  5.50024398e-02  3.13795917e-02  3.39484923e-02\n",
      " -3.54247428e-02  8.34668055e-02  9.88800973e-02  7.27545889e-03\n",
      " -6.68658456e-03 -7.65812490e-03  7.93738440e-02  7.39685493e-04\n",
      "  1.49292080e-02 -1.51047120e-02  3.67674418e-02  4.78743315e-02\n",
      " -4.81969416e-02 -3.76052186e-02 -4.60278317e-02 -8.89816210e-02\n",
      "  1.20228127e-01  1.30663246e-01 -3.73936258e-02  2.47858418e-03\n",
      "  2.55825138e-03  7.25814775e-02 -6.80436492e-02 -5.24696223e-02\n",
      "  4.90234084e-02  2.99563147e-02 -5.84429577e-02 -2.02263389e-02\n",
      "  2.08822098e-02  9.76691768e-02  3.52390558e-02  3.91140617e-02\n",
      "  1.05667887e-02  1.56232447e-03 -1.30822910e-02  8.52902979e-03\n",
      " -4.84092580e-03 -2.03766990e-02 -2.71800924e-02  2.83307396e-02\n",
      "  3.66017818e-02  2.51276158e-02 -9.90862101e-02  1.15626520e-02\n",
      " -3.60380299e-02 -7.23783895e-02 -1.12670109e-01  1.12942141e-02\n",
      " -3.86397280e-02  4.67386246e-02 -2.88460776e-02  2.26703621e-02\n",
      " -8.52407143e-03  3.32815312e-02 -1.06580253e-03 -7.09745586e-02\n",
      " -6.31170049e-02 -5.72186671e-02 -6.16026297e-02  5.47146536e-02\n",
      "  1.18317679e-02 -4.66261394e-02  2.56959721e-02 -7.07413070e-03\n",
      " -5.73842674e-02  4.12839390e-02 -5.91503307e-02  5.89021817e-02\n",
      " -4.41697426e-02  4.65081595e-02 -3.15814726e-02  5.58312573e-02\n",
      "  5.54578304e-02 -5.96533343e-02  4.06407081e-02  4.83764661e-03\n",
      " -4.96768020e-02 -1.00944355e-01  3.40078436e-02  4.13272297e-03\n",
      " -2.93525308e-03  2.11837627e-02 -3.73962186e-02 -2.79066861e-02\n",
      " -4.61768284e-02  5.26138432e-02 -2.79734824e-02 -1.62379220e-01\n",
      "  6.61042035e-02  1.72274187e-02 -5.45112276e-03  4.74474020e-02\n",
      " -3.82237397e-02 -3.96896675e-02  1.34544782e-02  4.49654162e-02\n",
      "  4.53674328e-03  2.82978732e-02  8.36633071e-02 -1.00858007e-02\n",
      " -1.19354002e-01 -3.84624302e-02  4.82858531e-02 -9.46083739e-02\n",
      "  1.91854239e-02 -9.96518433e-02 -6.30596504e-02  3.02695855e-02\n",
      "  1.17402561e-02 -4.78372611e-02 -6.20266423e-03 -3.32850702e-02\n",
      " -4.04388411e-03  1.28307045e-02  4.05253954e-02  7.56476894e-02\n",
      "  2.92434897e-02  2.84270402e-02 -2.78939027e-02  1.66858025e-02\n",
      " -2.47961897e-02 -6.83650821e-02  2.89968867e-02 -5.39867674e-33\n",
      " -2.69014505e-03 -2.65069269e-02 -6.47944340e-04 -8.46194942e-03\n",
      " -7.35154524e-02  4.94085532e-03 -5.97842261e-02  1.03438245e-02\n",
      "  2.12904206e-03 -2.88217561e-03 -3.17076594e-02 -9.42364335e-02\n",
      "  3.03020384e-02  7.00226426e-02  4.50685471e-02  3.69439572e-02\n",
      "  1.13593573e-02  3.53026874e-02  5.50452294e-03  1.34418136e-03\n",
      "  3.46123381e-03  7.75047839e-02  5.45112342e-02 -7.92055950e-02\n",
      " -9.31696817e-02 -4.03398573e-02  3.10668591e-02 -3.83081622e-02\n",
      " -5.89443296e-02  1.93331745e-02 -2.67159790e-02 -7.91938156e-02\n",
      "  1.04178835e-04  7.70621300e-02  4.16603535e-02  8.90932381e-02\n",
      "  3.56843323e-02 -1.09153166e-02  3.71498354e-02 -2.07070336e-02\n",
      " -2.46100444e-02 -2.05025394e-02  2.62201224e-02  3.43590602e-02\n",
      "  4.39251103e-02 -8.20519403e-03 -8.40710104e-02  4.24170829e-02\n",
      "  4.87499014e-02  5.95384724e-02  2.87747663e-02  3.37638259e-02\n",
      " -4.07442898e-02 -1.66367879e-03  7.91927576e-02  3.41088362e-02\n",
      " -5.72817342e-04  1.87749658e-02 -1.36963939e-02  7.38333166e-02\n",
      "  5.74477366e-04  8.33505243e-02  5.60811087e-02 -1.13710957e-02\n",
      "  4.42611426e-02  2.69581750e-02 -4.80535887e-02 -3.15087400e-02\n",
      "  7.75226429e-02  1.81773100e-02 -8.83005261e-02 -7.85519183e-03\n",
      " -6.22242838e-02  7.19372854e-02 -2.33475491e-02  6.52483711e-03\n",
      " -9.49526951e-03 -9.88312960e-02  4.01306413e-02  3.07397284e-02\n",
      " -2.21606977e-02 -9.45911407e-02  1.02367904e-02  1.02187783e-01\n",
      " -4.12959903e-02 -3.15778069e-02  4.74752039e-02 -1.10209785e-01\n",
      "  1.69615168e-02 -3.71709168e-02 -1.03261918e-02 -4.72538434e-02\n",
      " -1.20213823e-02 -1.93255134e-02  5.79292215e-02  4.23865814e-34\n",
      "  3.92013341e-02  8.41361880e-02 -1.02946714e-01  6.92259818e-02\n",
      "  1.68821551e-02 -3.26760598e-02  9.65962373e-03  1.80899762e-02\n",
      "  2.17939839e-02  1.63189322e-02 -9.69292223e-02  3.74850561e-03\n",
      " -2.38456707e-02 -3.44056003e-02  7.11962357e-02  9.21911385e-04\n",
      " -6.23858999e-03  3.23754400e-02 -8.90347990e-04  5.01905754e-03\n",
      " -4.24538068e-02  9.89083871e-02 -4.60320935e-02  4.69704829e-02\n",
      " -1.75284259e-02 -7.02515524e-03  1.32743875e-02 -5.30152246e-02\n",
      "  2.66406895e-03  1.45819075e-02  7.43345357e-03 -3.07131689e-02\n",
      " -2.09416300e-02  8.24110135e-02 -5.15893921e-02 -2.71178335e-02\n",
      "  1.17583029e-01  7.72505067e-03 -1.89523045e-02  3.94559205e-02\n",
      "  7.17360601e-02  2.59116627e-02  2.75191665e-02  9.50542092e-03\n",
      " -3.02355494e-02 -4.07944806e-02 -1.04028478e-01 -7.97422975e-03\n",
      " -3.64460330e-03  3.29716541e-02 -2.35954728e-02 -7.50518963e-03\n",
      " -5.82234189e-02 -3.17906067e-02 -4.18049432e-02  2.17453595e-02\n",
      " -6.67292178e-02 -4.89104167e-02  4.58516506e-03 -2.66046226e-02\n",
      " -1.12597018e-01  5.11167571e-02  5.48534282e-02 -6.69857115e-02\n",
      "  1.26766339e-01 -8.59487429e-02 -5.94231449e-02 -2.92188302e-03\n",
      " -1.14875613e-02 -1.26025781e-01 -3.48280789e-03 -9.12001953e-02\n",
      " -1.22933060e-01  1.33777270e-02 -4.75774929e-02 -6.57933131e-02\n",
      " -3.39409895e-02 -3.07107549e-02 -5.22033907e-02 -2.35464219e-02\n",
      "  5.90035245e-02 -3.85757871e-02  3.19701284e-02  4.05118428e-02\n",
      "  1.67077798e-02 -3.58280987e-02  1.45688038e-02  3.20137665e-02\n",
      " -1.34843402e-02  6.07819818e-02 -8.31398554e-03 -1.08105680e-02\n",
      "  4.69410866e-02  7.66133815e-02 -4.23400179e-02 -2.11963318e-08\n",
      " -7.25292563e-02 -4.20228206e-02 -6.12374581e-02  5.24666943e-02\n",
      " -1.42363701e-02  1.18487161e-02 -1.40788984e-02 -3.67530361e-02\n",
      " -4.44977470e-02 -1.15140183e-02  5.23316823e-02  2.96651535e-02\n",
      " -4.62780520e-02 -3.70892920e-02  1.89129598e-02  2.04306841e-02\n",
      " -2.24006064e-02 -1.48562919e-02 -1.79504156e-02  4.20007482e-02\n",
      "  1.40942633e-02 -2.83492208e-02 -1.16863027e-01  1.48956822e-02\n",
      " -7.30585307e-04  5.66028357e-02 -2.68740263e-02  1.09106667e-01\n",
      "  2.94560217e-03  1.19267881e-01  1.14212438e-01  8.92973468e-02\n",
      " -1.70255471e-02 -4.99053784e-02 -2.11930703e-02  3.18421274e-02\n",
      "  7.03436211e-02 -1.02929458e-01  8.23816806e-02  2.81968042e-02\n",
      "  3.21146548e-02  3.79108153e-02 -1.09553136e-01  8.19620192e-02\n",
      "  8.73216912e-02 -5.73563837e-02 -2.01709103e-02 -5.69444634e-02\n",
      " -1.30338334e-02 -5.55684417e-02 -1.32966265e-02  8.64010397e-03\n",
      "  5.30012175e-02 -4.06847373e-02  2.71708909e-02 -2.55947676e-03\n",
      "  3.05775627e-02 -4.61865142e-02  4.68035461e-03 -3.64947133e-02\n",
      "  6.80802912e-02  6.65087774e-02  8.49152282e-02 -3.32849398e-02]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 4.39335145e-02  5.89343905e-02  4.81784083e-02  7.75481388e-02\n",
      "  2.67443974e-02 -3.76295820e-02 -2.60508177e-03 -5.99430427e-02\n",
      " -2.49601854e-03  2.20728368e-02  4.80259433e-02  5.57552874e-02\n",
      " -3.89454030e-02 -2.66167857e-02  7.69340061e-03 -2.62376424e-02\n",
      " -3.64160910e-02 -3.78161222e-02  7.40781128e-02 -4.95050326e-02\n",
      " -5.85217103e-02 -6.36196584e-02  3.24350148e-02  2.20085755e-02\n",
      " -7.10637793e-02 -3.31577845e-02 -6.94104135e-02 -5.00373729e-02\n",
      "  7.46267885e-02 -1.11133814e-01 -1.23063065e-02  3.77456471e-02\n",
      " -2.80313455e-02  1.45353889e-02 -3.15585621e-02 -8.05836469e-02\n",
      "  5.83525524e-02  2.59005814e-03  3.92802171e-02  2.57695448e-02\n",
      "  4.98505943e-02 -1.75627891e-03 -4.55297716e-02  2.92607509e-02\n",
      " -1.02017239e-01  5.22287674e-02 -7.90899843e-02 -1.02857556e-02\n",
      "  9.20250453e-03  1.30732553e-02 -4.04778123e-02 -2.77925581e-02\n",
      "  1.24667529e-02  6.72832653e-02  6.81247711e-02 -7.57117802e-03\n",
      " -6.09941641e-03 -4.23776805e-02  5.17816246e-02 -1.56707577e-02\n",
      "  9.56359692e-03  4.12390344e-02  2.14959029e-02  1.04293507e-02\n",
      "  2.73349900e-02  1.87062416e-02 -2.69607287e-02 -7.00542331e-02\n",
      " -1.04700461e-01 -1.89873273e-03  1.77017115e-02 -5.74725494e-02\n",
      " -1.44223366e-02  4.70465282e-04  2.33228947e-03 -2.51920689e-02\n",
      "  4.93004061e-02 -5.09609766e-02  6.31983057e-02  1.49165252e-02\n",
      " -2.70766672e-02 -4.52875681e-02 -4.90594357e-02  3.74940485e-02\n",
      "  3.84579822e-02  1.56903197e-03  3.09922397e-02  2.01630499e-02\n",
      " -1.24363732e-02 -3.06719951e-02 -2.78819185e-02 -6.89182952e-02\n",
      " -5.13677001e-02  2.14795712e-02  1.15747023e-02  1.25411106e-03\n",
      "  1.88765451e-02 -4.42318693e-02 -4.49817665e-02 -3.41872219e-03\n",
      "  1.31131355e-02  2.00099535e-02  1.21099770e-01  2.31074709e-02\n",
      " -2.20159441e-02 -3.28846872e-02 -3.15509457e-03  1.17858137e-04\n",
      "  9.91499126e-02  1.65239442e-02 -4.69674077e-03 -1.45366918e-02\n",
      " -3.71076073e-03  9.65136364e-02  2.85907984e-02  2.13481747e-02\n",
      " -7.17645437e-02 -2.41142046e-02 -4.40940820e-02 -1.07346892e-01\n",
      "  6.79946020e-02  1.30466759e-01 -7.97029510e-02  6.79506641e-03\n",
      " -2.37511545e-02 -4.61636744e-02 -2.99650766e-02 -3.69410083e-33\n",
      "  7.30969980e-02 -2.20171418e-02 -8.61464590e-02 -7.14379698e-02\n",
      " -6.36741817e-02 -7.21863359e-02 -5.93043631e-03 -2.33641695e-02\n",
      " -2.83658262e-02  4.77434918e-02 -8.06176737e-02 -1.56475778e-03\n",
      "  1.38443140e-02 -2.86236126e-02 -3.35386544e-02 -1.13777533e-01\n",
      " -9.17635020e-03 -1.08101266e-02  3.23195979e-02  5.88380881e-02\n",
      "  3.34208682e-02  1.07987918e-01 -3.72712873e-02 -2.96770632e-02\n",
      "  5.17189838e-02 -2.25338638e-02 -6.96090832e-02 -2.14475468e-02\n",
      " -2.33411007e-02  4.82199714e-02 -3.58766131e-02 -4.68991399e-02\n",
      " -3.97873819e-02  1.10813215e-01 -1.43007077e-02 -1.18464492e-01\n",
      "  5.82915284e-02 -6.25889227e-02 -2.94041168e-02  6.03238903e-02\n",
      " -2.44411966e-03  1.60116181e-02  2.67233830e-02  2.49530543e-02\n",
      " -6.49318844e-02 -1.06801726e-02  2.81465072e-02  1.03563759e-02\n",
      " -6.63604238e-04  1.98186394e-02 -3.04288026e-02  6.28422387e-03\n",
      "  5.15268371e-02 -4.75375168e-02 -6.44421130e-02  9.55031812e-02\n",
      "  7.55858198e-02 -2.81574633e-02 -3.49965990e-02  1.01816386e-01\n",
      "  1.98732633e-02 -3.68036777e-02  2.93526798e-03 -5.00745028e-02\n",
      "  1.50932133e-01 -6.16079755e-02 -8.58812630e-02  7.13991327e-03\n",
      " -1.33065870e-02  7.80405328e-02  1.75250880e-02  4.21278961e-02\n",
      "  3.57939936e-02 -1.32950440e-01  3.56970169e-02 -2.03116853e-02\n",
      "  1.24910194e-02 -3.80355231e-02  4.91543449e-02 -1.56540908e-02\n",
      "  1.21418238e-01 -8.08644891e-02 -4.68782037e-02  4.10843305e-02\n",
      " -1.84318125e-02  6.69690743e-02  4.33596177e-03  2.27315091e-02\n",
      " -1.36428960e-02 -4.53238934e-02 -3.92828882e-02 -6.29889499e-03\n",
      "  5.29609956e-02 -3.69064733e-02  7.11676776e-02  2.33343416e-33\n",
      "  1.05231375e-01 -4.81874086e-02  6.95918947e-02  6.56976402e-02\n",
      " -4.65149023e-02  5.14492579e-02 -1.24475360e-02  3.20872031e-02\n",
      " -9.23356265e-02  5.00932708e-02 -3.28876115e-02  1.39138782e-02\n",
      " -8.70235730e-04 -4.90905484e-03  1.03946380e-01  3.21623142e-04\n",
      "  5.28110415e-02 -1.17990533e-02  2.31565777e-02  1.31768323e-02\n",
      " -5.25962524e-02  3.26702446e-02  3.08649032e-04  6.41129017e-02\n",
      "  3.88500877e-02  5.88008538e-02  8.29793140e-02 -1.88149456e-02\n",
      " -2.26377342e-02 -1.00473642e-01 -3.83752473e-02 -5.88081628e-02\n",
      "  1.82420202e-03 -4.26995046e-02  2.50195190e-02  6.40059933e-02\n",
      " -3.77482846e-02 -6.83901133e-03 -2.54605012e-03 -9.76042822e-02\n",
      "  1.88476145e-02 -8.83187284e-04  1.73611976e-02  7.10790455e-02\n",
      "  3.30393277e-02  6.93422742e-03 -5.60523346e-02  5.14634736e-02\n",
      " -4.29542214e-02  4.60077412e-02 -8.78830161e-03  3.17289345e-02\n",
      "  4.93965819e-02  2.95190047e-02 -5.05192876e-02 -5.43186963e-02\n",
      "  1.49938409e-04 -2.76614390e-02  3.46878581e-02 -2.10890453e-02\n",
      "  1.38059622e-02  2.99886707e-02  1.39744570e-02 -4.26469697e-03\n",
      " -1.50337387e-02 -8.76094997e-02 -6.85053840e-02 -4.28141542e-02\n",
      "  7.76944831e-02 -7.10285679e-02 -7.37689389e-03  2.13727150e-02\n",
      "  1.35562401e-02 -7.90464804e-02  5.47664380e-03  8.30663443e-02\n",
      "  1.14148006e-01  1.80759223e-03  8.75490978e-02 -4.16045003e-02\n",
      "  1.55416336e-02 -1.01206554e-02 -7.32439803e-03  1.07966093e-02\n",
      " -6.62816837e-02  3.98414135e-02 -1.16711520e-01  6.42994121e-02\n",
      "  4.02920097e-02 -6.54741600e-02  1.95052736e-02  8.09995756e-02\n",
      "  5.36463857e-02  7.67969862e-02 -1.34852491e-02 -1.76919031e-08\n",
      " -4.43935394e-02  9.20644309e-03 -8.79590213e-02  4.26921584e-02\n",
      "  7.31365085e-02  1.68427434e-02 -4.03262973e-02  1.85131431e-02\n",
      "  8.44172090e-02 -3.74477282e-02  3.02996207e-02  2.90641636e-02\n",
      "  6.36878684e-02  2.89750490e-02 -1.47269536e-02  1.77543107e-02\n",
      " -3.36895250e-02  1.73160974e-02  3.37874927e-02  1.76826030e-01\n",
      " -1.75533295e-02 -6.03077784e-02 -1.43394666e-02 -2.38536429e-02\n",
      " -4.45531420e-02 -2.89850123e-02 -8.96776393e-02 -1.75937277e-03\n",
      " -2.61486229e-02  5.94000239e-03 -5.18355146e-02  8.57279748e-02\n",
      " -8.18398893e-02  8.35437048e-03  4.00789715e-02  4.17764038e-02\n",
      "  1.04573555e-01 -2.86564534e-03  1.96691193e-02  5.81046613e-03\n",
      "  1.33253001e-02  4.51001301e-02 -2.17588283e-02 -1.39493234e-02\n",
      " -6.86992928e-02 -2.94114184e-03 -3.10764872e-02 -1.05854452e-01\n",
      "  6.91624284e-02 -4.24114615e-02 -4.67682071e-02 -3.64751071e-02\n",
      "  4.50399965e-02  6.09816872e-02 -6.56561404e-02 -5.45641780e-03\n",
      " -1.86226498e-02 -6.31484538e-02 -3.87436859e-02  3.46733965e-02\n",
      "  5.55458255e-02  5.21627851e-02  5.61065227e-02  1.02063932e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met een pretrained model kunnen we aantonen wat de mate van similarity is tussen twee zinnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.6153]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "emb1 = model.encode(\"This is a red cat with a hat.\")\n",
    "emb2 = model.encode(\"Have you seen my red cat?\")\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity test met eigen input. Lagere mate van similarity dan de voorbeeldzinnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity: tensor([[0.4199]])\n"
     ]
    }
   ],
   "source": [
    "emb1 = model.encode(\"This is a rotten apple\")\n",
    "emb2 = model.encode(\"Do these apples have blotches\")\n",
    "\n",
    "cos_sim = util.cos_sim(emb1, emb2)\n",
    "print(\"Cosine-Similarity:\", cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met onderstaande code kunnen we een top-5 van meest vergelijkbare zinnen samenstellen en bijbehorende percentages tonen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A man is eating food. \t A man is eating a piece of bread. \t 0.7553\n",
      "A man is riding a horse. \t A man is riding a white horse on an enclosed ground. \t 0.7369\n",
      "A monkey is playing drums. \t Someone in a gorilla costume is playing a set of drums. \t 0.6433\n",
      "A woman is playing violin. \t Someone in a gorilla costume is playing a set of drums. \t 0.2564\n",
      "A man is eating food. \t A man is riding a horse. \t 0.2474\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.'\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))\n",
    "# See on the left the Usage sections for more examples how to use SentenceTransformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onderstaand met zelf geformuleerde zinnen om te kijken of het patroon waar naar gezocht wordt enigszins duidelijk kan worden. Het lijkt er sterk op dat meerdere key words of variaties (enkelvoud-meervoud) daarop het snelst triggeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "A healthy apple doesn't have blotches. \t Some of the apple diseases are: rot, blotch and scab. \t 0.7044\n",
      "This apple has rot. \t Some of the apple diseases are: rot, blotch and scab. \t 0.6864\n",
      "A healthy apple doesn't have blotches. \t Some apples with scabs do not disqualify the complete batch. \t 0.6364\n",
      "Some of the apple diseases are: rot, blotch and scab. \t Some apples with scabs do not disqualify the complete batch. \t 0.6289\n",
      "This apple has rot. \t A healthy apple doesn't have blotches. \t 0.6095\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This apple has rot.\",\n",
    "          \"A healthy apple doesn't have blotches.\",\n",
    "          \"Damaged fruits are used to make apple butter.\",\n",
    "          \"Some of the apple diseases are: rot, blotch and scab.\",\n",
    "          \"An apples grows on a tree.\",\n",
    "          \"Fallen fruit might be spoiled.\",\n",
    "          \"The batch has been classified as Class 1\",\n",
    "          \"The batch is not even good enough for apple syrup\",\n",
    "          \"Apples don't drive cars.\",\n",
    "          \"Pears and apples can't be compared, but can be compeared, in [MASK] numbers.\",\n",
    "          \"Some apples with scabs do not disqualify the complete batch.\"\n",
    "          ]\n",
    "\n",
    "#Encode all sentences\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Compute cosine similarity between all pairs\n",
    "cos_sim = util.cos_sim(embeddings, embeddings)\n",
    "\n",
    "#Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "#Sort list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "   InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5627, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')                   # -> Similarity: tensor([[0.5627, 0.5645]])\n",
    "# model_original = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1') -> Similarity: tensor([[0.5472, 0.6330]])\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.dot_score(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We kunnen argmax() gebruiken om de best scorende waarde te selecteren.<br> \n",
    "Wanneer we de lokatie van deze waarde zoeken, kunnen we deze als antwoord printen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The batch has been classified as Class 1\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "sentencesOptimized = [\n",
    "                    \"This batch has been classified as {class x}.\",\n",
    "                    \"This batch has been classified as {class x}.\",\n",
    "                    \"This batch has been classified as {class x}.\",\n",
    "                    \"This batch has been classified as {class x}.\",\n",
    "                    \"The batch has been rejected.\",\n",
    "                    \"The batch is completely unsuitable and will be composted.\",\n",
    "                    \"The batch contains {appleScore} healthy apples, this is {healthyPercentage}% of the batch\",\n",
    "                    \"The batch contains {rotApple} rotten apples, this is {rotPercentage}% of the batch\",\n",
    "                    \"The batch contains {blotchApple} apples with blotch, this is {blotchPercentage}% of the batch\",\n",
    "                    \"The batch contains {scabApple} apples with scabs, this is {scabPercentage}% of the batch\",\n",
    "                    \"The batch has been rejected. With {rotApple} rotten, {blotchApple} blotched and {scabApple} scabbed apples, {rejectedPercentage} % is unsuitable.\"\n",
    "                    ]\n",
    "\n",
    "embeddingsApples = model.encode(sentencesOptimized)\n",
    "\n",
    "query_embedding = input('How can we help you?')\n",
    "passage_embedding = model.encode(query_embedding, convert_to_tensor=True)\n",
    "answer_array = util.dot_score(passage_embedding, embeddingsApples)\n",
    "\n",
    "# print(\"Similarity:\", util.dot_score(passage_embedding, embeddings))\n",
    "\n",
    "answer_location = answer_array.argmax()\n",
    "\n",
    "# print(answer_location)\n",
    "# print(answer_location.item())\n",
    "\n",
    "# de x-ste zin heeft de hoogste score\n",
    "\n",
    "print(sentences[answer_location.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning model keuze : all-MiniLM-L6-v2 is 5 keer sneller (dan het beste model: all-mpnet-base-v2) en geeft nog steeds een goede kwaliteit. Een model dat werkt op basis van symmetric semantic search (sss) is hier de beste oplossing, want vraag en antwoord zijn beide relatief kort en bondig, <i>sss</i> zoekt naar vergelijkbare vragen.bbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: A man is eating pasta.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A man is eating food. (Score: 0.7035)\n",
      "A man is eating a piece of bread. (Score: 0.5272)\n",
      "A man is riding a horse. (Score: 0.1889)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1047)\n",
      "A cheetah is running behind its prey. (Score: 0.0980)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Someone in a gorilla costume is playing a set of drums.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A monkey is playing drums. (Score: 0.6433)\n",
      "A woman is playing violin. (Score: 0.2564)\n",
      "A man is riding a horse. (Score: 0.1389)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1191)\n",
      "A cheetah is running behind its prey. (Score: 0.1080)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: A cheetah chases prey on across a field.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A cheetah is running behind its prey. (Score: 0.8253)\n",
      "A man is eating food. (Score: 0.1399)\n",
      "A monkey is playing drums. (Score: 0.1292)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.1097)\n",
      "A man is riding a horse. (Score: 0.0650)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'A cheetah is running behind its prey.'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
    "\n",
    "    \"\"\"\n",
    "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
    "    # hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    # hits = hits[0]      #Get the hits for the first query\n",
    "    # for hit in hits:\n",
    "    #     print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "    \"\"\"\n",
    "    \n",
    "    # hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    # hits = hits[0]      #Get the hits for the first query\n",
    "    # for hit in hits:\n",
    "    #     print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Met util.semantic_search:</strong><br>\n",
    "<ul>\n",
    "<li><strong>Query:</strong> A man is eating pasta.</li><br>\n",
    "<li>A man is eating food. (Score: 0.7035)</li>\n",
    "<li>A man is eating a piece of bread. (Score: 0.5272)</li>\n",
    "<li>A man is riding a horse. (Score: 0.1889)</li>\n",
    "<li>A man is riding a white horse on an enclosed ground. (Score: 0.1047)</li>\n",
    "<li>A cheetah is running behind its prey. (Score: 0.0980)</li><br>\n",
    "<li><strong>Query:</strong> Someone in a gorilla costume is playing a set of drums.</li><br>\n",
    "<li>A monkey is playing drums. (Score: 0.6433)</li>\n",
    "<li>A woman is playing violin. (Score: 0.2564)</li>\n",
    "<li>A man is riding a horse. (Score: 0.1389)</li>\n",
    "<li>A man is riding a white horse on an enclosed ground. (Score: 0.1191)</li>\n",
    "<li>A cheetah is running behind its prey. (Score: 0.1080)</li><br>\n",
    "<li><strong>Query:</strong> Someone in a gorilla costume is playing a set of drums.</li><br>\n",
    "<li>A cheetah is running behind its prey. (Score: 0.8253)</li>\n",
    "<li>A man is eating food. (Score: 0.1399)</li>\n",
    "<li>A monkey is playing drums. (Score: 0.1292)</li>\n",
    "<li>A man is riding a white horse on an enclosed ground. (Score: 0.1097)</li>\n",
    "<li>A man is riding a horse. (Score: 0.0650)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Met cosine-similarity en torch.topk:</strong><br>\n",
    "<ul>\n",
    "<li><strong>Query:</strong> A man is eating pasta.</li><br>\n",
    "<li><i>Top 5 most similar sentences in corpus:</i></li>\n",
    "<li>A man is eating food. (Score: 0.7035)</li>\n",
    "<li>A man is eating a piece of bread. (Score: 0.5272)</li>\n",
    "<li>A man is riding a horse. (Score: 0.1889)</li>\n",
    "<li>A man is riding a white horse on an enclosed ground. (Score: 0.1047)</li>\n",
    "<li>A cheetah is running behind its prey. (Score: 0.0980)</li><br>\n",
    "\n",
    "<li><strong>Query:</strong> Someone in a gorilla costume is playing a set of drums.</li><br>\n",
    "<li><i>Top 5 most similar sentences in corpus:</i></li>\n",
    "<li>A monkey is playing drums. (Score: 0.6433)</li>\n",
    "<li>A woman is playing violin. (Score: 0.2564)</li>\n",
    "<li>A man is riding a horse. (Score: 0.1389)</li>\n",
    "<li>A man is riding a white horse on an enclosed ground. (Score: 0.1191)</li>\n",
    "<li>A cheetah is running behind its prey. (Score: 0.1080)</li><br>\n",
    "\n",
    "<li><strong>Query:</strong> Someone in a gorilla costume is playing a set of drums.</li><br>\n",
    "<li><i>Top 5 most similar sentences in corpus:</i></li>\n",
    "<li>A cheetah is running behind its prey. (Score: 0.8253)</li>\n",
    "<li>A man is eating food. (Score: 0.1399)</li>\n",
    "<li>A monkey is playing drums. (Score: 0.1292)</li>\n",
    "<li>A man is riding a white horse on an enclosed ground. (Score: 0.1097)</li>\n",
    "<li>A man is riding a horse. (Score: 0.0650)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Resultaten <i>cosine-similarity/torch.topk</i> en <i>util.semantic_search</i> zijn identiek.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eens kijken wat er gebeurt als we eigen vragen en antwoorden m.b.t. tot onze kwaliteitscontrole gebruiken.<br>\n",
    "Met <i>top_k = min(1, len(corpus))</i> zouden we het beste antwoord kunnen selecteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: What is the quality of this batch?\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "The batch has been rejected. (Score: 0.4820)\n",
      "This batch contains 2 blotched apples. (Score: 0.4491)\n",
      "A batch can consist of healthy, rotten, blotched or scabbed apples (Score: 0.4385)\n",
      "The complete batch consists of 80 apples. (Score: 0.4252)\n",
      "The batch is categorized as Class 1. (Score: 0.3779)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: How many scabbed apples are in this batch?\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A batch can consist of healthy, rotten, blotched or scabbed apples (Score: 0.7868)\n",
      "This batch contains 2 blotched apples. (Score: 0.7694)\n",
      "The complete batch consists of 80 apples. (Score: 0.7507)\n",
      "4 Rotten apples spoil the batch. (Score: 0.6157)\n",
      "98%\\ of the apples is healthy. (Score: 0.5433)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: What is the percentage of healthy apples?\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "The percentage of healthy apples is 97.3% (Score: 0.9325)\n",
      "98%\\ of the apples is healthy. (Score: 0.9088)\n",
      "Apples are a healthy fruit. (Score: 0.7947)\n",
      "A batch can consist of healthy, rotten, blotched or scabbed apples (Score: 0.6077)\n",
      "The complete batch consists of 80 apples. (Score: 0.6046)\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = ['This batch contains 2 blotched apples.',\n",
    "          'The percentage of healthy apples is 97.3%',\n",
    "          'The complete batch consists of 80 apples.',\n",
    "          '98%\\ of the apples is healthy.',\n",
    "          '4 Rotten apples spoil the batch.',\n",
    "          'A batch can consist of healthy, rotten, blotched or scabbed apples',\n",
    "          'Apples are a healthy fruit.',\n",
    "          'The batch is categorized as Class 1.',\n",
    "          'The batch has been rejected.'\n",
    "          ]\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = ['What is the quality of this batch?', \n",
    "           'How many scabbed apples are in this batch?', \n",
    "           'What is the percentage of healthy apples?']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        print(corpus[idx], \"(Score: {:.4f})\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><strong>Dit zijn de resultaten van de eerst test:</strong></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><strong>Query:</strong> What is the quality of this batch?</li><br>\n",
    "\n",
    "<li><i>Top 5 most similar sentences in corpus:</i></li><br>\n",
    "<li>The batch has been rejected. (Score: 0.48200) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:MediumSeaGreen;\">Top! Wel lage score...</span></li>\n",
    "<li>This batch contains 2 blotched apples. (Score: 0.4491) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:DodgerBlue;\">Zegt iets over kwaliteit, maar niet specifiek genoeg.</span></li>\n",
    "<li>A batch can consist of healthy, rotten, blotched or scabbed apples (Score: 0.4385) &nbsp;&nbsp;&nbsp;&nbsp;<span style=color:Tomato> Zegt niet wat de kwaliteit is, wel hoe deze tot stand komt.</span></li>\n",
    "<li>The complete batch consists of 80 apples. (Score: 0.4252) &nbsp;&nbsp;&nbsp;&nbsp;<span style=color:Tomato> Hoeveelheid, niet kwaliteit.</span></li>\n",
    "<li>The batch is categorized as Class 1. (Score: 0.3697) &nbsp;&nbsp;&nbsp;&nbsp;<span style=color:Tomato> Deze zou veel hoger moeten scoren.</span></li>\n",
    "\n",
    "======================\n",
    "\n",
    "<li><strong>Query:</strong> How many scabbed apples are in this batch?</li><br>\n",
    "\n",
    "<li><i>Top 5 most similar sentences in corpus:</i></li><br>\n",
    "<li>A batch can consist of healthy, rotten, blotched or scabbed apples (Score: 0.7868) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:DodgerBlue;\">Zegt alleen dat er ook 'scabbed' appels in de batch <i>kunnen</i> zitten.</span></li>\n",
    "<li>This batch contains 2 blotched apples. (Score: 0.7694) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:DodgerBlue;\">'Blotched', niet 'scabbed'.</span></li>\n",
    "<li>The complete batch consists of 80 apples. (Score: 0.7507) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:DodgerBlue;\">Zegt iets over totaal, niets over 'scabbed' appels.</span></li>\n",
    "<li>4 Rotten apples spoil the batch. (Score: 0.6157) &nbsp;&nbsp;&nbsp;&nbsp; <span style=color:Tomato> 'Rotten', niet 'scabbed'.</span></li>\n",
    "<li>98% of the apples is healthy. (Score: 0.5437) &nbsp;&nbsp;&nbsp;&nbsp; <span style=color:Tomato> 'Healthy', niet 'scabbed'.</span></li>\n",
    "\n",
    "======================\n",
    "\n",
    "<li><strong>Query:</strong> What is the percentage of healthy apples?</li><br>\n",
    "\n",
    "<li><i>Top 5 most similar sentences in corpus:</i></li><br>\n",
    "<li>The percentage of healthy apples is 97.3% (Score: 0.9325) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:MediumSeaGreen;\">Top!</span></li>\n",
    "<li>98\"%\" of the apples is healthy. (Score: 0.9070) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:MediumSeaGreen;\">Top!</span></li>\n",
    "<li>Apples are a healthy fruit. (Score: 0.7947) &nbsp;&nbsp;&nbsp;&nbsp; <span style=\"color:DodgerBlue;\">Zegt niets over het percentage.</span></li>\n",
    "<li>A batch can consist of healthy, rotten, blotched or scabbed apples (Score: 0.6077) &nbsp;&nbsp;&nbsp;&nbsp; <span style=color:Tomato> Zegt niets over het percentage.</span></li>\n",
    "<li>The complete batch consists of 80 apples. (Score: 0.6046) &nbsp;&nbsp;&nbsp;&nbsp; <span style=color:Tomato> Zegt niets over gezonde appels of percentage.</span></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m query_embedding \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39mencode(query, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m corpus_embeddings \u001b[39m=\u001b[39m embedder\u001b[39m.\u001b[39mencode(corpus, convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m answer_array \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39;49mdot_score(corpus_embeddings, corpus)\n\u001b[0;32m      7\u001b[0m \u001b[39m# print(\"Similarity:\", util.dot_score(passage_embedding, embeddings))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m answer_location \u001b[39m=\u001b[39m answer_array\u001b[39m.\u001b[39margmax()\n",
      "File \u001b[1;32mc:\\MakeAIWork2\\env\\lib\\site-packages\\sentence_transformers\\util.py:61\u001b[0m, in \u001b[0;36mdot_score\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     58\u001b[0m     a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(a)\n\u001b[0;32m     60\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(b, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m---> 61\u001b[0m     b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(b)\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(a\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     64\u001b[0m     a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "answer_array = util.dot_score(corpus_embeddings, corpus)\n",
    "\n",
    "# print(\"Similarity:\", util.dot_score(passage_embedding, embeddings))\n",
    "\n",
    "answer_location = answer_array.argmax()\n",
    "\n",
    "# print(answer_location)\n",
    "# print(answer_location.item())\n",
    "\n",
    "# de x-ste zin heeft de hoogste score\n",
    "\n",
    "print(sentences[answer_location.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><strong>En nu?</strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoe kunnen we de zelfgemaakte vragen en antwoorden beter op elkaar laten aansluiten?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-transformers==1.0.4, torch==1.7.0.\n",
    "# import random\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset\n",
    "from sentence_transformers.losses import TripletLoss\n",
    "from sentence_transformers.readers import LabelSentenceReader, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load pre-trained model - we are using the original Sentence-BERT for this example. / \n",
    "# 'all-MiniLM-L6-v2' < eerder gebruikt, werkt dit ook?\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Set up data for fine-tuning \n",
    "sentence_reader = LabelSentenceReader(folder='C:/MakeAIWork2/projects/apple_disease_classification/notebooks/')\n",
    "data_list = sentence_reader.get_examples(filename='sbert_first.tsv')\n",
    "triplets = triplets_from_labeled_dataset(input_examples=data_list)\n",
    "finetune_data = SentencesDataset(examples=triplets, model=sbert_model)\n",
    "finetune_dataloader = DataLoader(finetune_data, shuffle=True, batch_size=16)\n",
    "\n",
    "# Initialize triplet loss\n",
    "loss = TripletLoss(model=sbert_model)\n",
    "\n",
    "# Fine-tune the model\n",
    "sbert_model.fit(train_objectives=[(finetune_dataloader, loss)], epochs=4,output_path='all-MiniLM-L6-v2-disease')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3806e804db5aa4a4c842d8541c0221916f65eb25aef94211593b7aec60c2bed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
